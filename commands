download the ollama = huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir vllm_workspace/Llama-3.1-8B-Instruct --local-dir-use-symlinks False

docker build -t triton-vllm-guardrails .